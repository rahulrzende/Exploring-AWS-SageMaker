{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize stuff for project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import sagemaker # Amazon Sagemaker Python SDK (needed for using prebuilt Sagemaker models/algos)\n",
    "import boto3     # AWS Python SDK(helps us read data from S3 buckets, just like pandas helps us read from CSVs, etc.)\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input, Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "# initialize S3 bucket name and detect region for creation\n",
    "bucket_name = \"rzapplication1\" # simply the bucket name for our application, can be IAM controlled/restricted down the road\n",
    "my_region = boto3.session.Session().region_name\n",
    "print(my_region)              # we check our region name, for use with S3 bucket that is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket ' rzapplication1 ' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# create S3 bucket programmatically\n",
    "s3 = boto3.resource(\"s3\")\n",
    "try: \n",
    "    if my_region == \"us-west-2\":\n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': 'us-west-2'})\n",
    "        print(\"S3 bucket '\",  bucket_name,\"' created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"S3 error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://rzapplication1/sagemaker_xgboost_model/output\n"
     ]
    }
   ],
   "source": [
    "# mapping for storage in S3 bucket\n",
    "model_name_prefix = \"sagemaker_xgboost_model\"\n",
    "model_write_path = \"s3://{}/{}/output\".format(bucket_name, model_name_prefix)\n",
    "print(model_write_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull and ready data for training/testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load additional libraries\n",
    "import pandas as pd\n",
    "import urllib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded bank_clean.csv.\n"
     ]
    }
   ],
   "source": [
    "# download the dataset via specified URL\n",
    "try:\n",
    "    urllib.request.urlretrieve(\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \n",
    "                                \"bank_clean.csv\")\n",
    "    print(\"Success: downloaded bank_clean.csv.\")\n",
    "except Exception as e:\n",
    "    print(\"Data download error: \", e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded file successfully loaded as a pandas dataframe!\n"
     ]
    }
   ],
   "source": [
    "# read that data into a pandas dataframe now\n",
    "try:\n",
    "    model_data = pd.read_csv(\"./bank_clean.csv\")\n",
    "    print(\"Success: downloaded file successfully loaded as a pandas dataframe!\")\n",
    "except Exception as e:\n",
    "    print(\"Pandas load error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data:  (28831, 62)\n",
      "Shape of test data:  (12357, 62)\n"
     ]
    }
   ],
   "source": [
    "# do the train-test split of our downloaded dataset, into a 70-30 ratio\n",
    "import numpy as np\n",
    "train_data, test_data = np.split(model_data.sample(frac = 1, random_state = 101), [int(0.7 * len(model_data))])\n",
    "print(\"Shape of train data: \", train_data.shape)\n",
    "print(\"Shape of test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# save the train data into S3 bucket\n",
    "import os\n",
    "\n",
    "# convert into the data format required by Sagemaker, with response first and then the predictors\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', \n",
    "                                                                                            index=False, \n",
    "                                                                                            header=False)\n",
    "\n",
    "# upload to S3 bucket we created above\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(model_name_prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "\n",
    "# load the object with reference to this newly uploaded train dataset\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket_name, model_name_prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# save the test data into S3 bucket\n",
    "import os\n",
    "\n",
    "# convert into the data format required by Sagemaker, with response first and then the predictors\n",
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', \n",
    "                                                                                            index=False, \n",
    "                                                                                            header=False)\n",
    "\n",
    "# upload to S3 bucket we created above\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(model_name_prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "\n",
    "# load the object with reference to this newly uploaded test dataset\n",
    "s3_input_test = sagemaker.s3_input(s3_data='s3://{}/{}/test'.format(bucket_name, model_name_prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build XGBoost model using Sagemaker algo and data from created S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# the Sagemaker pre-built algos are retrieved as containers, and we need to pull them into our current AS instance as follows\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version=\"1.0-1\") # last argument is for fetching latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# initialize a model object using our container\n",
    "xgb_model = sagemaker.estimator.Estimator(image_name=container, # The container image to use for training\n",
    "                                         role=sagemaker.get_execution_role(), # An AWS IAM role (either name or full ARN) \n",
    "                                         train_instance_count=1, # Number of Amazon EC2 instances to use for training\n",
    "                                         train_instance_type='ml.m5.2xlarge', # Type of EC2 instance to use for training\n",
    "                                         train_volume_size = 5, # Size (GB) of EBS volume to use for storing I/P data during training\n",
    "                                         output_path=model_write_path, # S3 location for saving the training result (model artifacts and output files)\n",
    "                                         sagemaker_session=sagemaker.Session(), # Session object which manages interactions with Amazon SageMaker APIs and any other AWS services needed\n",
    "                                         train_use_spot_instances=True, # Specifies whether to use SageMaker Managed Spot instances for training\n",
    "                                         train_max_run=300, # Timeout in seconds for training (After this amount of time Amazon SageMaker terminates the job regardless of its current status)\n",
    "                                         train_max_wait=600) #  Timeout in seconds waiting for spot training instances\n",
    "\n",
    "# up above, the last 3 parameteres were set in order to reduce the amount AWS bills to my credit card :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify our custom set of hyperparameters for our model\n",
    "xgb_model.set_hyperparameters(max_depth = 5,\n",
    "                              eta = .2,\n",
    "                              gamma = 4,\n",
    "                              min_child_weight = 6,\n",
    "                              subsample = 0.7, # Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees (this will prevent overfitting). Subsampling will occur once in every boosting iteration.\n",
    "                              silent = 0,\n",
    "                              objective = \"binary:logistic\",\n",
    "                              num_round = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-11 21:43:59 Starting - Starting the training job...\n",
      "2020-10-11 21:44:01 Starting - Launching requested ML instances......\n",
      "2020-10-11 21:45:03 Starting - Preparing the instances for training......\n",
      "2020-10-11 21:46:29 Downloading - Downloading input data\n",
      "2020-10-11 21:46:29 Training - Training image download completed. Training in progress..\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[21:46:31] 28831x60 matrix with 1729860 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[21:46:31] 12357x60 matrix with 741420 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 28831 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 12357 rows\u001b[0m\n",
      "\u001b[34m[21:46:31] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.09757#011validation-error:0.10253\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.09642#011validation-error:0.10269\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.09504#011validation-error:0.10075\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.09524#011validation-error:0.10059\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.09524#011validation-error:0.10067\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.09511#011validation-error:0.10067\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.09521#011validation-error:0.10075\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.09472#011validation-error:0.10124\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.09462#011validation-error:0.10148\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.09462#011validation-error:0.10100\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.09448#011validation-error:0.10075\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.09417#011validation-error:0.10100\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.09410#011validation-error:0.10140\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.09396#011validation-error:0.10091\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.09417#011validation-error:0.10124\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.09396#011validation-error:0.10116\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.09351#011validation-error:0.10124\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.09341#011validation-error:0.10116\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.09341#011validation-error:0.10124\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.09320#011validation-error:0.10148\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.09323#011validation-error:0.10164\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.09299#011validation-error:0.10164\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.09334#011validation-error:0.10116\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.09337#011validation-error:0.10132\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.09323#011validation-error:0.10116\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.09337#011validation-error:0.10116\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.09306#011validation-error:0.10100\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.09302#011validation-error:0.10124\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.09295#011validation-error:0.10075\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.09257#011validation-error:0.10059\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.09271#011validation-error:0.10059\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.09268#011validation-error:0.10059\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.09240#011validation-error:0.10100\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.09233#011validation-error:0.10108\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.09230#011validation-error:0.10108\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.09223#011validation-error:0.10100\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.09261#011validation-error:0.10116\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.09219#011validation-error:0.10100\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.09178#011validation-error:0.10075\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.09198#011validation-error:0.10100\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.09219#011validation-error:0.10091\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.09216#011validation-error:0.10140\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.09202#011validation-error:0.10172\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.09198#011validation-error:0.10091\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.09143#011validation-error:0.10091\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.09143#011validation-error:0.10067\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.09136#011validation-error:0.10108\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.09143#011validation-error:0.10091\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.09150#011validation-error:0.10124\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.09150#011validation-error:0.10132\u001b[0m\n",
      "\n",
      "2020-10-11 21:46:42 Uploading - Uploading generated training model\n",
      "2020-10-11 21:46:42 Completed - Training job completed\n",
      "Training seconds: 32\n",
      "Billable seconds: 15\n",
      "Managed Spot Training savings: 53.1%\n"
     ]
    }
   ],
   "source": [
    "# fit the model to our data, provide the paths (URLs) to training and validation datasets\n",
    "xgb_model.fit({'train': s3_input_train,'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# Deploy this trained model using endpoints\n",
    "\n",
    "xgb_predictor = xgb_model.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the endpoint will be accepting data in a typical tabular format, we set up things for that\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the labels from input dataset\n",
    "input_data = test_data.drop(['y_no','y_yes'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the expected type of input for our endpoint model\n",
    "xgb_predictor.content_type = 'text/csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly, set the serializer type for our endpoint\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab predictions using trained model endpoint for passed input data\n",
    "predictions = xgb_predictor.predict(input_data).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the output in proper format (array)\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09114921, 0.05947307, 0.07003157, ..., 0.03261118, 0.01985421,\n",
       "       0.05698879])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Classification Rate: 89.9%\n",
      "\n",
      "Predicted      No Purchase    Purchase\n",
      "Observed\n",
      "No Purchase    91% (10726)    34% (198)\n",
      "Purchase        9% (1054)     66% (379) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"No Purchase\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Purchase\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the end point etc. to prevent additional billing - need to convert to a markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': '54271468171E6E19',\n",
       "   'HostId': '0MGQHrfuA+pRnuUusKcuCyt/fIHXh9X/veBOVPit15wQastx72ZDMHlHAhSEFbtXvcgukDkReIo=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': '0MGQHrfuA+pRnuUusKcuCyt/fIHXh9X/veBOVPit15wQastx72ZDMHlHAhSEFbtXvcgukDkReIo=',\n",
       "    'x-amz-request-id': '54271468171E6E19',\n",
       "    'date': 'Mon, 12 Oct 2020 06:05:57 GMT',\n",
       "    'connection': 'close',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'sagemaker_xgboost_model/output/sagemaker-xgboost-2020-10-11-21-43-59-350/output/model.tar.gz'},\n",
       "   {'Key': 'sagemaker_xgboost_model/train/train.csv'},\n",
       "   {'Key': 'sagemaker_xgboost_model/test/test.csv'},\n",
       "   {'Key': 'sagemaker_xgboost_model/output/sagemaker-xgboost-2020-10-05-05-31-11-801/output/model.tar.gz'}]}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}