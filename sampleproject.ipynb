{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize stuff for project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import sagemaker # Amazon Sagemaker Python SDK (needed for using prebuilt Sagemaker models/algos)\n",
    "import boto3     # AWS Python SDK(helps us read data from S3 buckets, just like pandas helps us read from CSVs, etc.)\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input, Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "# initialize S3 bucket name and detect region for creation\n",
    "bucket_name = \"rzapplication1\" # simply the bucket name for our application, can be IAM controlled/restricted down the road\n",
    "my_region = boto3.session.Session().region_name\n",
    "print(my_region)              # we check our region name, for use with S3 bucket that is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket ' rzapplication1 ' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# create S3 bucket programmatically\n",
    "s3 = boto3.resource(\"s3\")\n",
    "try: \n",
    "    if my_region == \"us-west-2\":\n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': 'us-west-2'})\n",
    "        print(\"S3 bucket '\",  bucket_name,\"' created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"S3 error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://rzapplication1/sagemaker_xgboost_model/output\n"
     ]
    }
   ],
   "source": [
    "# mapping for storage in S3 bucket\n",
    "model_name_prefix = \"sagemaker_xgboost_model\"\n",
    "model_write_path = \"s3://{}/{}/output\".format(bucket_name, model_name_prefix)\n",
    "print(model_write_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data for training/testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load additional libraries\n",
    "import pandas as pd\n",
    "import urllib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded bank_clean.csv.\n"
     ]
    }
   ],
   "source": [
    "# download the dataset via specified URL\n",
    "try:\n",
    "    urllib.request.urlretrieve(\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \n",
    "                                \"bank_clean.csv\")\n",
    "    print(\"Success: downloaded bank_clean.csv.\")\n",
    "except Exception as e:\n",
    "    print(\"Data download error: \", e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded file successfully loaded as a pandas dataframe!\n"
     ]
    }
   ],
   "source": [
    "# read that data into a pandas dataframe now\n",
    "try:\n",
    "    model_data = pd.read_csv(\"./bank_clean.csv\")\n",
    "    print(\"Success: downloaded file successfully loaded as a pandas dataframe!\")\n",
    "except Exception as e:\n",
    "    print(\"Pandas load error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data:  (28831, 62)\n",
      "Shape of test data:  (12357, 62)\n"
     ]
    }
   ],
   "source": [
    "# do the train-test split of our downloaded dataset, into a 70-30 ratio\n",
    "import numpy as np\n",
    "train_data, test_data = np.split(model_data.sample(frac = 1, random_state = 101), [int(0.7 * len(model_data))])\n",
    "print(\"Shape of train data: \", train_data.shape)\n",
    "print(\"Shape of test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# save the train data into S3 bucket\n",
    "import os\n",
    "\n",
    "# convert into the data format required by Sagemaker, with response first and then the predictors\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', \n",
    "                                                                                            index=False, \n",
    "                                                                                            header=False)\n",
    "\n",
    "# upload to S3 bucket we created above\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(model_name_prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "\n",
    "# load the object with reference to this newly uploaded train dataset\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket_name, model_name_prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# save the test data into S3 bucket\n",
    "import os\n",
    "\n",
    "# convert into the data format required by Sagemaker, with response first and then the predictors\n",
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', \n",
    "                                                                                            index=False, \n",
    "                                                                                            header=False)\n",
    "\n",
    "# upload to S3 bucket we created above\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(model_name_prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "\n",
    "# load the object with reference to this newly uploaded test dataset\n",
    "s3_input_test = sagemaker.s3_input(s3_data='s3://{}/{}/test'.format(bucket_name, model_name_prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build XGBoost model using Sagemaker algo and data from created S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Sagemaker pre-built algos are present as containers, and we need to pull them into our current AS instance as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
